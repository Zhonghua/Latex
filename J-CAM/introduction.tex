\section{Introduction}
\label{sec:intro}

The number of adaptive Finite Element codes is growing.
Let us mention (in alphabetical order): Alberta
\cite{alberta}, DealII \cite{dealii}, FEniCS
\cite{fenics}, FETK \cite{fetk}, Hermes \cite{hermes}, 
libMesh \cite{libmesh}, Phaml \cite{phaml}, PHG \cite{phg}, 
2dhp90 \cite{2dhp90} and many other commercial softwares.
%A natural question that arises is how they perform compared to each other?
%Unfortunately, comparison efforts are usually inhibited at the very beginning
%by diverse installation requirements, number of supporting libraries, non-unified
%input and output data formats, and different usage of various codes. And even if
%these problems can be overcome, there is not many benchmarks with known exact
%solutions that are able to test various aspects of automatic adaptivity in
%the appropriate manner.
There is no common approach to test all kinds of adaptive 
finite element algorithms by different application platforms, 
different software environment settings, 
different ways of loading physical model, different grid types, 
different methods of establishing the boundary conditions, 
different input/output formats as well as some empirical knowledge. 
In addition, the use of adaptive finite element analysis in special 
industries also limits the comparing of their accuracy and efficiency.
For instance, MSC.NASTRAN is an universal finite element analysis software, 
which is widely used in the aerospace industry, ANSYS, ABAQUS and ADINA are 
mostly used in the civil engineering, and Ansoft is mainly used in electromagnetic 
field analysis problem of electrical engineering and electronic engineering. 
Therefore, in order to compare the obtained results or to select a appropriate 
algorithm for one special problem, the evaluation of solving ability of finite 
element algorithm under the various problems are required. For some classical 
finite element problems, the pre-process methods of different software vary greatly. 
As a result, the same algorithm may get different results and convergent tendency 
for the same problem.

The criterion of performance for adaptive algorithms is to be
evaluated in terms of accuracy as a function of the total number
of DOF and CPU time. However, this is quite difficult to establish
for CPU time because the various softwares were run on different
computer platforms.
It is common to compare different algorithms using a large test set
with known exact solutions that are able to test various aspects of
automatic adaptivity in the appropriate manner. 
At this point we would like to acknowledge the work of
Dr. William Mitchell (NIST) who collected a suite of
twelve benchmarks for adaptive FEM \cite{mitchell-1}. 

In this paper, we presents a series of benchmarks problems 
with diverse solutions contained in \cite{mitchell-1}
that are designed to test the ability to handle different 
modes of singularities, near singularities and other difficulties
of adaptive algorithms for different equations.
The test problems and their solutions are formulated in Sections
\ref{sec:bench-1} - \ref{sec:bench-12} by using {\sc Hermes}
library (http://hpfem.org/hermes). {\sc Hermes} is a multi-platform 
open source C++ library for rapid development of adaptive $hp$-FEM 
and $hp$-DG solvers. In each section begins with a short description
of benchmark problem compared, then the numerical results are
presented and discussed. Conclusion and outlooks are offered
in the last section of this paper.


